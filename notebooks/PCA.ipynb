{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(2018)\n",
    "\n",
    "my_stop_words = STOPWORDS.union(set(['httpstco']))\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "punct_str = '''!\"$%&'()*+,-./:;<=>?[\\]^_`{|}~'''\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace('\\\\n',' ')\n",
    "    text = text.translate(str.maketrans(' ', ' ', punct_str))\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in my_stop_words and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./follower_twts/KnowledgeFight.csv' does not exist: b'./follower_twts/KnowledgeFight.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-96cb51261b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpodcast1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./follower_twts/KnowledgeFight.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpodcast2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./follower_twts/SeincastASeinfeldPodcast.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpodcast1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpodcast2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweets\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'[]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pod_tweets/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pod_tweets/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pod_tweets/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pod_tweets/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pod_tweets/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./follower_twts/KnowledgeFight.csv' does not exist: b'./follower_twts/KnowledgeFight.csv'"
     ]
    }
   ],
   "source": [
    "podcast1 = pd.read_csv('./pod_tweets/follower_twts/KnowledgeFight.csv')\n",
    "podcast2 = pd.read_csv('./pod_tweets/follower_twts/SeincastASeinfeldPodcast.csv')\n",
    "data = podcast1.append(podcast2, ignore_index=True)\n",
    "data = data[data.tweets != '[]']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8600\n",
      "                                              tweets  index\n",
      "0  ['Just a thought: if we replaced the police wi...      0\n",
      "1  ['What the actual fuck? https://t.co/RB6QH6AmK...      1\n",
      "3  ['@knowledge_fight Listening to your 14 Aug 20...      3\n",
      "4  ['@Communism_Kills Boring', '@JasonVarheinum @...      4\n",
      "5  ['@milesofgray sir, I feel dumb, but a guest f...      5\n"
     ]
    }
   ],
   "source": [
    "data_text = data[['tweets']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "\n",
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--original document: \n",
      "['Just a thought: if we replaced the police with furries, one immediate benefit we would see is less dead dogs.', \"@KelsCara @PugetSoundJBGC @IwriteOK said it the best in his latest episode of @bastardspod. Everyone knows there's something wrong and that they should be angry at something. It's just that folks like these have been tricked into thinking it's the left that is causing all their problems.\", '@BernieSanders https://t.co/v3ALhSdZhS', \"@IwriteOK There's something about a book. I can't bear to throw them away. I mourn the ones that have been lost in one of my many moves.\", \"@BillyWayneDavis Do you want a robot uprising? That's for sure how you get a robot uprising.\", '@TheZoneCast @peacockTV https://t.co/Pns0UvwDEf', '@katystoll Katy, we need you as a juror! Sane, logical people that understand how unfair the legal system is.', 'This will keep me goin for a goos bit more. https://t.co/PBAKBqdEK2', 'I’m telling racists to go back to Europe all 2020', '@bastardspod This excites me so much!']\n",
      "--rm punc:\n",
      "Just a thought if we replaced the police with furries one immediate benefit we would see is less dead dogs @KelsCara @PugetSoundJBGC @IwriteOK said it the best in his latest episode of @bastardspod Everyone knows theres something wrong and that they should be angry at something Its just that folks like these have been tricked into thinking its the left that is causing all their problems @BernieSanders httpstcov3ALhSdZhS @IwriteOK Theres something about a book I cant bear to throw them away I mourn the ones that have been lost in one of my many moves @BillyWayneDavis Do you want a robot uprising Thats for sure how you get a robot uprising @TheZoneCast @peacockTV httpstcoPns0UvwDEf @katystoll Katy we need you as a juror Sane logical people that understand how unfair the legal system is This will keep me goin for a goos bit more httpstcoPBAKBqdEK2 I’m telling racists to go back to Europe all 2020 @bastardspod This excites me so much\n",
      "--Separated:\n",
      "['Just', 'a', 'thought', 'if', 'we', 'replaced', 'the', 'police', 'with', 'furries', 'one', 'immediate', 'benefit', 'we', 'would', 'see', 'is', 'less', 'dead', 'dogs', '@KelsCara', '@PugetSoundJBGC', '@IwriteOK', 'said', 'it', 'the', 'best', 'in', 'his', 'latest', 'episode', 'of', '@bastardspod', 'Everyone', 'knows', 'theres', 'something', 'wrong', 'and', 'that', 'they', 'should', 'be', 'angry', 'at', 'something', 'Its', 'just', 'that', 'folks', 'like', 'these', 'have', 'been', 'tricked', 'into', 'thinking', 'its', 'the', 'left', 'that', 'is', 'causing', 'all', 'their', 'problems', '@BernieSanders', 'httpstcov3ALhSdZhS', '@IwriteOK', 'Theres', 'something', 'about', 'a', 'book', 'I', 'cant', 'bear', 'to', 'throw', 'them', 'away', 'I', 'mourn', 'the', 'ones', 'that', 'have', 'been', 'lost', 'in', 'one', 'of', 'my', 'many', 'moves', '@BillyWayneDavis', 'Do', 'you', 'want', 'a', 'robot', 'uprising', 'Thats', 'for', 'sure', 'how', 'you', 'get', 'a', 'robot', 'uprising', '@TheZoneCast', '@peacockTV', 'httpstcoPns0UvwDEf', '@katystoll', 'Katy', 'we', 'need', 'you', 'as', 'a', 'juror', 'Sane', 'logical', 'people', 'that', 'understand', 'how', 'unfair', 'the', 'legal', 'system', 'is', 'This', 'will', 'keep', 'me', 'goin', 'for', 'a', 'goos', 'bit', 'more', 'httpstcoPBAKBqdEK2', 'I’m', 'telling', 'racists', 'to', 'go', 'back', 'to', 'Europe', 'all', '2020', '@bastardspod', 'This', 'excites', 'me', 'so', 'much']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['think', 'replac', 'polic', 'furri', 'immedi', 'benefit', 'dead', 'dog', 'kelscara', 'pugetsoundjbgc', 'iwriteok', 'say', 'best', 'latest', 'episod', 'bastardspod', 'know', 'there', 'wrong', 'angri', 'folk', 'like', 'trick', 'think', 'leav', 'caus', 'problem', 'berniesand', 'httpstcov', 'alhsdzh', 'iwriteok', 'there', 'book', 'bear', 'throw', 'away', 'mourn', 'one', 'lose', 'move', 'billywaynedavi', 'want', 'robot', 'upris', 'that', 'sure', 'robot', 'upris', 'thezonecast', 'peacocktv', 'httpstcopn', 'uvwdef', 'katystol', 'kati', 'need', 'juror', 'sane', 'logic', 'peopl', 'understand', 'unfair', 'legal', 'goin', 'goo', 'tell', 'racist', 'europ', 'bastardspod', 'excit']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 0].values[0][0]\n",
    "\n",
    "punct_str = '''!\"$%&'()*+,-./:;<=>?[\\]^_`{|}~'''\n",
    "\n",
    "# WordNetLemmatizer().lemmatize(doc_sample, pos='v')\n",
    "print('--original document: ')\n",
    "doc_sample = doc_sample.replace('\\\\n',' ')\n",
    "print(doc_sample)\n",
    "print('--rm punc:')\n",
    "doc_sample = doc_sample.translate(str.maketrans(' ', ' ', punct_str))\n",
    "print(doc_sample)\n",
    "print('--Separated:')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [think, replac, polic, furri, immedi, benefit,...\n",
       "1     [actual, fuck, httpstcorb, yike, mean, know, f...\n",
       "3     [knowledgefight, listen, talk, ashyana, dean, ...\n",
       "4     [communismkil, bore, jasonvarheinum, cernovich...\n",
       "5     [milesofgray, feel, dumb, guest, fair, recent,...\n",
       "6     [good, block, work, memori, forget, password, ...\n",
       "7     [enter, chanc, custom, robeytech, coolermast, ...\n",
       "8     [care, endors, berni, sander, point, matter, c...\n",
       "11    [thedailybeast, aint, cheif, jkenney, like, di...\n",
       "12    [planter, tell, stori, death, peanut, zora, th...\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['tweets'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 alhsdzh\n",
      "1 angri\n",
      "2 away\n",
      "3 bastardspod\n",
      "4 bear\n",
      "5 benefit\n",
      "6 berniesand\n",
      "7 best\n",
      "8 billywaynedavi\n",
      "9 book\n",
      "10 caus\n",
      "dictionary len: 126733\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "print('dictionary len: {}'.format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary len: 2755\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=30, no_above=0.3, keep_n=100000)\n",
    "print('dictionary len: {}'.format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 angri\n"
     ]
    }
   ],
   "source": [
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(28, 2),\n",
       " (75, 1),\n",
       " (84, 1),\n",
       " (96, 1),\n",
       " (108, 1),\n",
       " (131, 1),\n",
       " (147, 2),\n",
       " (151, 1),\n",
       " (163, 1),\n",
       " (176, 1),\n",
       " (181, 1),\n",
       " (187, 1),\n",
       " (195, 1),\n",
       " (222, 1),\n",
       " (231, 1),\n",
       " (253, 2),\n",
       " (260, 1),\n",
       " (274, 1),\n",
       " (283, 1),\n",
       " (284, 1),\n",
       " (287, 1),\n",
       " (292, 2),\n",
       " (306, 1),\n",
       " (313, 1),\n",
       " (349, 1),\n",
       " (353, 1),\n",
       " (374, 1),\n",
       " (384, 1),\n",
       " (405, 1),\n",
       " (420, 1),\n",
       " (497, 1),\n",
       " (501, 1),\n",
       " (511, 1),\n",
       " (517, 1),\n",
       " (551, 1),\n",
       " (562, 1),\n",
       " (651, 1),\n",
       " (695, 1),\n",
       " (696, 1),\n",
       " (703, 1),\n",
       " (725, 1),\n",
       " (745, 2),\n",
       " (757, 1),\n",
       " (797, 1),\n",
       " (800, 1),\n",
       " (811, 1),\n",
       " (833, 1),\n",
       " (850, 1),\n",
       " (938, 1),\n",
       " (953, 1),\n",
       " (994, 1),\n",
       " (1024, 1),\n",
       " (1056, 1),\n",
       " (1080, 1),\n",
       " (1087, 1),\n",
       " (1107, 1),\n",
       " (1130, 1),\n",
       " (1175, 1),\n",
       " (1186, 1),\n",
       " (1210, 1),\n",
       " (1216, 2),\n",
       " (1240, 1),\n",
       " (1275, 1),\n",
       " (1281, 1),\n",
       " (1313, 1),\n",
       " (1324, 1),\n",
       " (1331, 1),\n",
       " (1397, 1),\n",
       " (1487, 2),\n",
       " (1594, 1),\n",
       " (1601, 2),\n",
       " (1626, 1),\n",
       " (1710, 1),\n",
       " (1721, 1),\n",
       " (2064, 1),\n",
       " (2132, 1),\n",
       " (2257, 1),\n",
       " (2273, 1),\n",
       " (2349, 2),\n",
       " (2375, 1),\n",
       " (2392, 1)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 28 (\"peopl\") appears 2 time.\n",
      "Word 75 (\"carri\") appears 1 time.\n",
      "Word 84 (\"final\") appears 1 time.\n",
      "Word 96 (\"promis\") appears 1 time.\n",
      "Word 108 (\"year\") appears 1 time.\n",
      "Word 131 (\"announc\") appears 1 time.\n",
      "Word 147 (\"give\") appears 2 time.\n",
      "Word 151 (\"help\") appears 1 time.\n",
      "Word 163 (\"pete\") appears 1 time.\n",
      "Word 176 (\"tri\") appears 1 time.\n",
      "Word 181 (\"absolut\") appears 1 time.\n",
      "Word 187 (\"christma\") appears 1 time.\n",
      "Word 195 (\"httpstcof\") appears 1 time.\n",
      "Word 222 (\"celebr\") appears 1 time.\n",
      "Word 231 (\"evil\") appears 1 time.\n",
      "Word 253 (\"live\") appears 2 time.\n",
      "Word 260 (\"nation\") appears 1 time.\n",
      "Word 274 (\"spend\") appears 1 time.\n",
      "Word 283 (\"valley\") appears 1 time.\n",
      "Word 284 (\"watch\") appears 1 time.\n",
      "Word 287 (\"week\") appears 1 time.\n",
      "Word 292 (\"world\") appears 2 time.\n",
      "Word 306 (\"hous\") appears 1 time.\n",
      "Word 313 (\"presid\") appears 1 time.\n",
      "Word 349 (\"role\") appears 1 time.\n",
      "Word 353 (\"state\") appears 1 time.\n",
      "Word 374 (\"death\") appears 1 time.\n",
      "Word 384 (\"honor\") appears 1 time.\n",
      "Word 405 (\"statement\") appears 1 time.\n",
      "Word 420 (\"debt\") appears 1 time.\n",
      "Word 497 (\"possibl\") appears 1 time.\n",
      "Word 501 (\"american\") appears 1 time.\n",
      "Word 511 (\"constitut\") appears 1 time.\n",
      "Word 517 (\"hold\") appears 1 time.\n",
      "Word 551 (\"complet\") appears 1 time.\n",
      "Word 562 (\"grow\") appears 1 time.\n",
      "Word 651 (\"pray\") appears 1 time.\n",
      "Word 695 (\"heart\") appears 1 time.\n",
      "Word 696 (\"hero\") appears 1 time.\n",
      "Word 703 (\"littl\") appears 1 time.\n",
      "Word 725 (\"unit\") appears 1 time.\n",
      "Word 745 (\"father\") appears 2 time.\n",
      "Word 757 (\"tax\") appears 1 time.\n",
      "Word 797 (\"cover\") appears 1 time.\n",
      "Word 800 (\"fear\") appears 1 time.\n",
      "Word 811 (\"pass\") appears 1 time.\n",
      "Word 833 (\"depress\") appears 1 time.\n",
      "Word 850 (\"debat\") appears 1 time.\n",
      "Word 938 (\"comfort\") appears 1 time.\n",
      "Word 953 (\"creat\") appears 1 time.\n",
      "Word 994 (\"true\") appears 1 time.\n",
      "Word 1024 (\"truli\") appears 1 time.\n",
      "Word 1056 (\"readi\") appears 1 time.\n",
      "Word 1080 (\"beauti\") appears 1 time.\n",
      "Word 1087 (\"georg\") appears 1 time.\n",
      "Word 1107 (\"voter\") appears 1 time.\n",
      "Word 1130 (\"staff\") appears 1 time.\n",
      "Word 1175 (\"cancer\") appears 1 time.\n",
      "Word 1186 (\"save\") appears 1 time.\n",
      "Word 1210 (\"cast\") appears 1 time.\n",
      "Word 1216 (\"govern\") appears 2 time.\n",
      "Word 1240 (\"walk\") appears 1 time.\n",
      "Word 1275 (\"season\") appears 1 time.\n",
      "Word 1281 (\"holiday\") appears 1 time.\n",
      "Word 1313 (\"young\") appears 1 time.\n",
      "Word 1324 (\"potenti\") appears 1 time.\n",
      "Word 1331 (\"finish\") appears 1 time.\n",
      "Word 1397 (\"die\") appears 1 time.\n",
      "Word 1487 (\"pain\") appears 2 time.\n",
      "Word 1594 (\"whitehous\") appears 1 time.\n",
      "Word 1601 (\"light\") appears 2 time.\n",
      "Word 1626 (\"rais\") appears 1 time.\n",
      "Word 1710 (\"fals\") appears 1 time.\n",
      "Word 1721 (\"address\") appears 1 time.\n",
      "Word 2064 (\"duti\") appears 1 time.\n",
      "Word 2132 (\"part\") appears 1 time.\n",
      "Word 2257 (\"suffer\") appears 1 time.\n",
      "Word 2273 (\"shadow\") appears 1 time.\n",
      "Word 2349 (\"cure\") appears 2 time.\n",
      "Word 2375 (\"even\") appears 1 time.\n",
      "Word 2392 (\"bush\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(\n",
    "            bow_doc_4310[i][0], dictionary[bow_doc_4310[i][0]], bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.17350044118710398),\n",
      " (1, 0.09933709763028145),\n",
      " (2, 0.36277649009686225),\n",
      " (3, 0.13273029030096511),\n",
      " (4, 0.16391450080290204),\n",
      " (5, 0.14462607848588757),\n",
      " (6, 0.07694368848606319),\n",
      " (7, 0.10897447863651102),\n",
      " (8, 0.1306301333571147),\n",
      " (9, 0.1325866245361997),\n",
      " (10, 0.16263225517441865),\n",
      " (11, 0.10239518200268184),\n",
      " (12, 0.19870211772407972),\n",
      " (13, 0.12200989557763872),\n",
      " (14, 0.13360383723039101),\n",
      " (15, 0.1454207575513756),\n",
      " (16, 0.16048709148144993),\n",
      " (17, 0.2491899220729396),\n",
      " (18, 0.18875717972085881),\n",
      " (19, 0.04956622360964723),\n",
      " (20, 0.1510338232847544),\n",
      " (21, 0.08895324949750409),\n",
      " (22, 0.15734242484404992),\n",
      " (23, 0.18517863072712085),\n",
      " (24, 0.09628401138610057),\n",
      " (25, 0.13682912790003307),\n",
      " (26, 0.06042565213969678),\n",
      " (27, 0.14855758305081965),\n",
      " (28, 0.05009031210129303),\n",
      " (29, 0.144040749115548),\n",
      " (30, 0.11989615728578926),\n",
      " (31, 0.14790924083322554),\n",
      " (32, 0.16952359100702066),\n",
      " (33, 0.06329896985689006),\n",
      " (34, 0.08747481885526978),\n",
      " (35, 0.07737961569320295),\n",
      " (36, 0.09049130283634797),\n",
      " (37, 0.2442374416050701),\n",
      " (38, 0.09707833423677861),\n",
      " (39, 0.1328744992268133),\n",
      " (40, 0.19068077463619512),\n",
      " (41, 0.11543781961878659),\n",
      " (42, 0.2110523979663578),\n",
      " (43, 0.05935953423524417),\n",
      " (44, 0.11087411842464627)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=6, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.011*\"time\" + 0.010*\"follow\" + 0.009*\"game\" + 0.008*\"want\" + 0.007*\"chanc\" + 0.007*\"know\" + 0.007*\"think\" + 0.006*\"watch\" + 0.006*\"need\" + 0.006*\"good\"\n",
      "Topic: 1 \n",
      "Words: 0.012*\"think\" + 0.010*\"great\" + 0.008*\"peopl\" + 0.007*\"look\" + 0.007*\"time\" + 0.006*\"realdonaldtrump\" + 0.006*\"say\" + 0.006*\"know\" + 0.005*\"work\" + 0.005*\"year\"\n",
      "Topic: 2 \n",
      "Words: 0.013*\"peopl\" + 0.010*\"think\" + 0.009*\"year\" + 0.008*\"know\" + 0.006*\"trump\" + 0.006*\"time\" + 0.005*\"want\" + 0.005*\"good\" + 0.005*\"need\" + 0.005*\"go\"\n",
      "Topic: 3 \n",
      "Words: 0.012*\"year\" + 0.008*\"know\" + 0.007*\"time\" + 0.007*\"today\" + 0.007*\"love\" + 0.006*\"team\" + 0.006*\"happi\" + 0.006*\"great\" + 0.006*\"right\" + 0.006*\"think\"\n",
      "Topic: 4 \n",
      "Words: 0.011*\"podcast\" + 0.010*\"go\" + 0.009*\"episod\" + 0.008*\"time\" + 0.008*\"love\" + 0.008*\"game\" + 0.006*\"play\" + 0.006*\"good\" + 0.006*\"year\" + 0.006*\"come\"\n",
      "Topic: 5 \n",
      "Words: 0.010*\"peopl\" + 0.010*\"trump\" + 0.007*\"say\" + 0.007*\"berni\" + 0.007*\"know\" + 0.007*\"want\" + 0.007*\"time\" + 0.006*\"thing\" + 0.006*\"good\" + 0.006*\"right\"\n",
      "Topic: 6 \n",
      "Words: 0.013*\"trump\" + 0.011*\"thank\" + 0.009*\"need\" + 0.008*\"love\" + 0.007*\"go\" + 0.006*\"retweet\" + 0.006*\"help\" + 0.006*\"think\" + 0.006*\"say\" + 0.006*\"peopl\"\n",
      "Topic: 7 \n",
      "Words: 0.012*\"work\" + 0.011*\"peopl\" + 0.009*\"year\" + 0.008*\"thank\" + 0.007*\"today\" + 0.007*\"go\" + 0.007*\"time\" + 0.006*\"come\" + 0.006*\"thing\" + 0.005*\"think\"\n",
      "Topic: 8 \n",
      "Words: 0.008*\"think\" + 0.007*\"live\" + 0.006*\"know\" + 0.006*\"love\" + 0.006*\"good\" + 0.005*\"come\" + 0.005*\"time\" + 0.005*\"look\" + 0.005*\"week\" + 0.005*\"go\"\n",
      "Topic: 9 \n",
      "Words: 0.013*\"know\" + 0.012*\"look\" + 0.009*\"time\" + 0.009*\"think\" + 0.008*\"good\" + 0.007*\"year\" + 0.007*\"love\" + 0.006*\"go\" + 0.005*\"right\" + 0.005*\"movi\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.003*\"peopl\" + 0.003*\"think\" + 0.003*\"know\" + 0.003*\"podcast\" + 0.003*\"time\" + 0.003*\"go\" + 0.003*\"year\" + 0.003*\"love\" + 0.003*\"thank\" + 0.002*\"need\"\n",
      "Topic: 1 Word: 0.003*\"love\" + 0.003*\"good\" + 0.003*\"think\" + 0.003*\"time\" + 0.003*\"year\" + 0.002*\"happi\" + 0.002*\"need\" + 0.002*\"right\" + 0.002*\"say\" + 0.002*\"peopl\"\n",
      "Topic: 2 Word: 0.004*\"trump\" + 0.003*\"peopl\" + 0.003*\"know\" + 0.002*\"think\" + 0.002*\"love\" + 0.002*\"time\" + 0.002*\"good\" + 0.002*\"year\" + 0.002*\"game\" + 0.002*\"fuck\"\n",
      "Topic: 3 Word: 0.003*\"love\" + 0.003*\"thank\" + 0.003*\"think\" + 0.003*\"year\" + 0.003*\"time\" + 0.003*\"know\" + 0.002*\"thing\" + 0.002*\"peopl\" + 0.002*\"look\" + 0.002*\"follow\"\n",
      "Topic: 4 Word: 0.003*\"trump\" + 0.003*\"love\" + 0.003*\"go\" + 0.003*\"peopl\" + 0.003*\"time\" + 0.003*\"year\" + 0.003*\"thank\" + 0.003*\"think\" + 0.003*\"game\" + 0.002*\"retweet\"\n",
      "Topic: 5 Word: 0.003*\"think\" + 0.003*\"game\" + 0.003*\"love\" + 0.003*\"look\" + 0.003*\"peopl\" + 0.003*\"know\" + 0.003*\"good\" + 0.002*\"time\" + 0.002*\"year\" + 0.002*\"thank\"\n",
      "Topic: 6 Word: 0.003*\"time\" + 0.003*\"go\" + 0.003*\"good\" + 0.003*\"think\" + 0.003*\"love\" + 0.003*\"peopl\" + 0.003*\"year\" + 0.003*\"game\" + 0.003*\"great\" + 0.003*\"know\"\n",
      "Topic: 7 Word: 0.003*\"peopl\" + 0.003*\"trump\" + 0.003*\"time\" + 0.003*\"year\" + 0.003*\"think\" + 0.003*\"know\" + 0.002*\"thank\" + 0.002*\"love\" + 0.002*\"great\" + 0.002*\"come\"\n",
      "Topic: 8 Word: 0.003*\"retweet\" + 0.003*\"year\" + 0.003*\"good\" + 0.003*\"game\" + 0.003*\"look\" + 0.003*\"peopl\" + 0.002*\"episod\" + 0.002*\"think\" + 0.002*\"love\" + 0.002*\"know\"\n",
      "Topic: 9 Word: 0.003*\"thank\" + 0.003*\"know\" + 0.003*\"love\" + 0.003*\"want\" + 0.003*\"need\" + 0.003*\"time\" + 0.003*\"think\" + 0.003*\"game\" + 0.002*\"realdonaldtrump\" + 0.002*\"year\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
