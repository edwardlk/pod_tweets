{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import gensim\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AceOnTheHouse', '83WeekswithEricBischoff', '48Hours', 'Accused', '99Invisible', 'AdamCarollaShow', '2DopeQueens', '1YearDailyAudioBible', '1A', '30For30Podcasts', '60Minutes', 'PlanetMoney', 'AccidentalTechPodcast', 'PodSaveAmerica', 'AliceIsntDead', 'AlisonRosenIsYourNewBestFriend', 'BehindtheBastards', 'SeincastASeinfeldPodcast', 'KnowledgeFight']\n"
     ]
    }
   ],
   "source": [
    "# data_dir = '/home/ed/github/pod_tweets/follower_twts/'\n",
    "data_dir = '/run/media/ed/SD/follower_twts/'\n",
    "categories = os.listdir(data_dir)\n",
    "print(categories)\n",
    "\n",
    "docs_to_train = sklearn.datasets.load_files(data_dir, description=None,\n",
    "                                            categories=categories, load_content=True,\n",
    "                                            encoding='utf-8', shuffle=True,\n",
    "                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 13, 18, ..., 12,  7,  8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the loaded dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(docs_to_train.data, docs_to_train.target, test_size=0.2)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = X_train[6:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Input test:\n",
      "[\"@saladinahmed Black Bolt is one of the best books at Marvel right now. I can't wait for this.\", '@FanBrosShow Just figured out what to do with all the #ComicsICopped this year. https://t.co/hZjWpdLRKS', '@jaggedlittlehil Stop.', '@jaggedlittlehil This is so scary.', \"@eveewing It was great to see you perform this in NYC. I'm excited for the book!\", 'Really proud that my great uncle is a founding member of this group.\\nhttps://t.co/T3cTDNWqmc', '@FanBrosShow Just finished all of the Patternist books. Really hard not to imagine an amazing Inhumans story there.', \"America's slow but very real decline into a fascist state as told by the Milwaukee Bucks logo https://t.co/UTVsEyq3kg\", \"@ulabeast I totally remember working on this as an intern in Summer '10.\", 'Deadass. :( https://t.co/QysTPMlLEG']\n",
      "::Processed text:\n",
      "black bolt best book marvel right wait figur year stop scari great perform excit book proud great uncl member group finish patternist book hard imagin amaz inhuman stori america slow real declin fascist state tell milwauk buck logo total rememb work intern summer deadass #ComicsICopped @saladinahmed @FanBrosShow @jaggedlittlehil @jaggedlittlehil @eveewing @FanBrosShow @ulabeast\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import re\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import preprocessor as p\n",
    "\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.EMOJI)\n",
    "stemmer = SnowballStemmer('english')\n",
    "punct_str = '''!\"$%&'()*+,-./:;<=>?[\\]^_`{|}~â€™'''\n",
    "# stop_words = STOPWORDS\n",
    "stop_words = STOPWORDS.union(set(['', 'ive', 'im', 'amp']))\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def my_preprocess(text):\n",
    "    '''\n",
    "    '''\n",
    "    doc_emoji = split_count(text)\n",
    "    doc_hash = find_hash(text)\n",
    "    doc_mentions = find_mention(text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\\\n',' ')\n",
    "    text = p.clean(text)\n",
    "    text = text.translate(str.maketrans(' ', ' ', punct_str))\n",
    "    text = re.sub(r' \\d+ ', ' ', text)\n",
    "    text = re.sub(r' \\d+ ', ' ', text)\n",
    "    words = []\n",
    "    for word in text.split(' '):\n",
    "        words.append(word)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    output = ' '.join(words)\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in stop_words and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    \n",
    "    result = result + doc_emoji + doc_hash + doc_mentions\n",
    "    tweet_txt = ' '.join(result)\n",
    "    return tweet_txt\n",
    "\n",
    "# HASHTAG_PATTERN = re.compile(r'#\\w*')\n",
    "# MENTION_PATTERN = re.compile(r'@\\w*')\n",
    "\n",
    "import emoji\n",
    "import regex\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list\n",
    "\n",
    "def find_hash(text):\n",
    "    hashtag_list = []\n",
    "    data = regex.findall(r'#\\w*', text)\n",
    "    for word in data:\n",
    "        hashtag_list.append(word)\n",
    "    return hashtag_list\n",
    "\n",
    "def find_mention(text):\n",
    "    mention_list = []\n",
    "    data = regex.findall(r'@\\w*', text)\n",
    "    for word in data:\n",
    "        mention_list.append(word)\n",
    "    return mention_list\n",
    "\n",
    "doc_sample = X_train[7]\n",
    "# doc_emoji = split_count(doc_sample)\n",
    "# doc_hash = find_hash(doc_sample)\n",
    "# doc_mentions = find_mention(doc_sample)\n",
    "print('::Input test:')\n",
    "print(doc_sample)\n",
    "\n",
    "doc_sample = my_preprocess(doc_sample)\n",
    "\n",
    "# all_words = doc_sample + doc_emoji + doc_hash + doc_mentions\n",
    "# tweet_txt = ' '.join(all_words)\n",
    "processed_txt = my_preprocess(doc_sample)\n",
    "print('::Processed text:')\n",
    "print(processed_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think power tattoo afraid underneath watch akira draw akira comic send help ahhhh look amaz handsom life dont know explain littl vivid composit life love inspir what poppin joint sleep entir life think dyinnggg ooz boiii long relationship wait accept ball sauc tast peopl draw slim fit black turtleneck draw slim fit black turtleneck draw slim fit black turtleneck draw slim fit black turtleneck draw slim fit black turtleneck draw @pynch__me @OneTrickTofani @JustinWabs @PancakeJoji @muwurder @_Cosmoetic_ @milkymosaic @hiya_cass @OneTrickTofani',\n",
       " 'black bolt best book marvel right wait figur year stop scari great perform excit book proud great uncl found member group finish patternist book hard imagin amaz inhuman stori america slow real declin fascist state tell milwauke buck logo total rememb work intern summer deadass #ComicsICopped @saladinahmed @FanBrosShow @jaggedlittlehil @jaggedlittlehil @eveewing @FanBrosShow @ulabeast']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reformat = lambda x: my_preprocess(x)\n",
    "test_list_m = list(map(reformat, test_list))\n",
    "test_list_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data into a form the classifier can work with\n",
    "vectorizer = CountVectorizer(token_pattern=r'[^\\s]+', encoding='unicode', lowercase=None, strip_accents=None, stop_words=None)\n",
    "reformat = lambda x: my_preprocess(x)\n",
    "X_train_m = list(map(reformat, X_train))\n",
    "X = vectorizer.fit_transform(X_train_m)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X)\n",
    "\n",
    "text_clf = Pipeline([('vect', vectorizer),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, \n",
    "    verbose=1)),])\n",
    "# text_clf.fit(X_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6b732b25535a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfidf_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count_vect' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_counts = count_vect.fit_transform(raw_documents=X_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "X_test_counts = count_vect.fit_transform(raw_documents=X_test)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Code\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, \n",
    "    verbose=1)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.28, NNZs: 104078, Bias: -1.004174, T: 52290, Avg. loss: 0.107010\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.12, NNZs: 138906, Bias: -1.003937, T: 104580, Avg. loss: 0.105449\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.07, NNZs: 168383, Bias: -1.002710, T: 156870, Avg. loss: 0.105255\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.05, NNZs: 192440, Bias: -1.002210, T: 209160, Avg. loss: 0.105150\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 211046, Bias: -1.001665, T: 261450, Avg. loss: 0.105096\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.03, NNZs: 225817, Bias: -1.001352, T: 313740, Avg. loss: 0.105057\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.03, NNZs: 237374, Bias: -1.001159, T: 366030, Avg. loss: 0.105033\n",
      "Total training time: 0.40 seconds.\n",
      "Convergence after 7 epochs took 0.41 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.36, NNZs: 107937, Bias: -1.005974, T: 52290, Avg. loss: 0.099852\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.24, NNZs: 139997, Bias: -1.002137, T: 104580, Avg. loss: 0.098526\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.21, NNZs: 161238, Bias: -1.001584, T: 156870, Avg. loss: 0.098307\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.19, NNZs: 175970, Bias: -1.000869, T: 209160, Avg. loss: 0.098215\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.19, NNZs: 186609, Bias: -1.000550, T: 261450, Avg. loss: 0.098157\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.18, NNZs: 194255, Bias: -1.000427, T: 313740, Avg. loss: 0.098120\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.18, NNZs: 200511, Bias: -1.000073, T: 366030, Avg. loss: 0.098100\n",
      "Total training time: 0.39 seconds.\n",
      "Convergence after 7 epochs took 0.39 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.14, NNZs: 98941, Bias: -1.005913, T: 52290, Avg. loss: 0.102316\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.96, NNZs: 133251, Bias: -1.004218, T: 104580, Avg. loss: 0.100854\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.90, NNZs: 162440, Bias: -1.003460, T: 156870, Avg. loss: 0.100662\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.88, NNZs: 186099, Bias: -1.002697, T: 209160, Avg. loss: 0.100570\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.87, NNZs: 205051, Bias: -1.001876, T: 261450, Avg. loss: 0.100514\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.86, NNZs: 220075, Bias: -1.001895, T: 313740, Avg. loss: 0.100477\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.85, NNZs: 231056, Bias: -1.001390, T: 366030, Avg. loss: 0.100455\n",
      "Total training time: 0.41 seconds.\n",
      "Convergence after 7 epochs took 0.41 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.19, NNZs: 93900, Bias: -1.005735, T: 52290, Avg. loss: 0.093864\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.05, NNZs: 123380, Bias: -1.003062, T: 104580, Avg. loss: 0.092553\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.01, NNZs: 145407, Bias: -1.002330, T: 156870, Avg. loss: 0.092370\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.99, NNZs: 163630, Bias: -1.000929, T: 209160, Avg. loss: 0.092287\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.99, NNZs: 177368, Bias: -1.000700, T: 261450, Avg. loss: 0.092240\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.98, NNZs: 187299, Bias: -1.000441, T: 313740, Avg. loss: 0.092207\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.98, NNZs: 196897, Bias: -1.000113, T: 366030, Avg. loss: 0.092186\n",
      "Total training time: 0.42 seconds.\n",
      "Convergence after 7 epochs took 0.42 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.17, NNZs: 98552, Bias: -1.005464, T: 52290, Avg. loss: 0.096908\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.00, NNZs: 133959, Bias: -1.002639, T: 104580, Avg. loss: 0.095605\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.95, NNZs: 164496, Bias: -1.002076, T: 156870, Avg. loss: 0.095435\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.93, NNZs: 191762, Bias: -1.001391, T: 209160, Avg. loss: 0.095346\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.91, NNZs: 213279, Bias: -1.000979, T: 261450, Avg. loss: 0.095291\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.91, NNZs: 230880, Bias: -1.000842, T: 313740, Avg. loss: 0.095258\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.90, NNZs: 245115, Bias: -1.000553, T: 366030, Avg. loss: 0.095236\n",
      "Total training time: 0.42 seconds.\n",
      "Convergence after 7 epochs took 0.42 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.15, NNZs: 134730, Bias: -1.002396, T: 52290, Avg. loss: 0.096199\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.99, NNZs: 173315, Bias: -1.001983, T: 104580, Avg. loss: 0.094888\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.94, NNZs: 203265, Bias: -1.001018, T: 156870, Avg. loss: 0.094703\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.92, NNZs: 227431, Bias: -1.000478, T: 209160, Avg. loss: 0.094616\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.91, NNZs: 247258, Bias: -1.000331, T: 261450, Avg. loss: 0.094566\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.91, NNZs: 263040, Bias: -0.999969, T: 313740, Avg. loss: 0.094533\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.90, NNZs: 275593, Bias: -0.999968, T: 366030, Avg. loss: 0.094510\n",
      "Total training time: 0.40 seconds.\n",
      "Convergence after 7 epochs took 0.41 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.35, NNZs: 94898, Bias: -1.005679, T: 52290, Avg. loss: 0.102794\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.26, NNZs: 122943, Bias: -1.004241, T: 104580, Avg. loss: 0.101411\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.24, NNZs: 141938, Bias: -1.002241, T: 156870, Avg. loss: 0.101254\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.23, NNZs: 156096, Bias: -1.001591, T: 209160, Avg. loss: 0.101150\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.23, NNZs: 166122, Bias: -1.000879, T: 261450, Avg. loss: 0.101125\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.23, NNZs: 173941, Bias: -1.000535, T: 313740, Avg. loss: 0.101080\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.23, NNZs: 180453, Bias: -1.000711, T: 366030, Avg. loss: 0.101047\n",
      "Total training time: 0.52 seconds.\n",
      "Convergence after 7 epochs took 0.52 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.18, NNZs: 127266, Bias: -1.005877, T: 52290, Avg. loss: 0.104401\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.01, NNZs: 171179, Bias: -1.003256, T: 104580, Avg. loss: 0.102990\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.95, NNZs: 209619, Bias: -1.002521, T: 156870, Avg. loss: 0.102801\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.93, NNZs: 239861, Bias: -1.001986, T: 209160, Avg. loss: 0.102706\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.92, NNZs: 262796, Bias: -1.001336, T: 261450, Avg. loss: 0.102649\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.91, NNZs: 279970, Bias: -1.001037, T: 313740, Avg. loss: 0.102612\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.91, NNZs: 293659, Bias: -1.000790, T: 366030, Avg. loss: 0.102585\n",
      "Total training time: 0.44 seconds.\n",
      "Convergence after 7 epochs took 0.44 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.47, NNZs: 126323, Bias: -1.006250, T: 52290, Avg. loss: 0.103088\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.35, NNZs: 164320, Bias: -1.004514, T: 104580, Avg. loss: 0.101714\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.32, NNZs: 192182, Bias: -1.002777, T: 156870, Avg. loss: 0.101541\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.30, NNZs: 210499, Bias: -1.002715, T: 209160, Avg. loss: 0.101441\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.30, NNZs: 224096, Bias: -1.001996, T: 261450, Avg. loss: 0.101379\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.29, NNZs: 234780, Bias: -1.001362, T: 313740, Avg. loss: 0.101342\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.29, NNZs: 244093, Bias: -1.001365, T: 366030, Avg. loss: 0.101320\n",
      "Total training time: 0.54 seconds.\n",
      "Convergence after 7 epochs took 0.54 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.55, NNZs: 99014, Bias: -1.007235, T: 52290, Avg. loss: 0.087957\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.44, NNZs: 133489, Bias: -1.004389, T: 104580, Avg. loss: 0.086729\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.41, NNZs: 160006, Bias: -1.004201, T: 156870, Avg. loss: 0.086576\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.40, NNZs: 179181, Bias: -1.003313, T: 209160, Avg. loss: 0.086492\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.39, NNZs: 193341, Bias: -1.003318, T: 261450, Avg. loss: 0.086432\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.39, NNZs: 204903, Bias: -1.002548, T: 313740, Avg. loss: 0.086409\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.39, NNZs: 213577, Bias: -1.002503, T: 366030, Avg. loss: 0.086378\n",
      "Total training time: 0.50 seconds.\n",
      "Convergence after 7 epochs took 0.50 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.35, NNZs: 86266, Bias: -1.005034, T: 52290, Avg. loss: 0.096742\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.21, NNZs: 112812, Bias: -1.002988, T: 104580, Avg. loss: 0.095447\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.17, NNZs: 133255, Bias: -1.001117, T: 156870, Avg. loss: 0.095278\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.15, NNZs: 149981, Bias: -1.000883, T: 209160, Avg. loss: 0.095182\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.14, NNZs: 163708, Bias: -1.000483, T: 261450, Avg. loss: 0.095137\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.14, NNZs: 174375, Bias: -1.000480, T: 313740, Avg. loss: 0.095098\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.13, NNZs: 184555, Bias: -1.000332, T: 366030, Avg. loss: 0.095081\n",
      "Total training time: 0.42 seconds.\n",
      "Convergence after 7 epochs took 0.43 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.34, NNZs: 89734, Bias: -1.003901, T: 52290, Avg. loss: 0.099553\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.18, NNZs: 118183, Bias: -1.002603, T: 104580, Avg. loss: 0.098181\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.14, NNZs: 142749, Bias: -1.001817, T: 156870, Avg. loss: 0.097989\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.12, NNZs: 164970, Bias: -1.001682, T: 209160, Avg. loss: 0.097910\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.11, NNZs: 183748, Bias: -1.000959, T: 261450, Avg. loss: 0.097867\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.10, NNZs: 199360, Bias: -1.000858, T: 313740, Avg. loss: 0.097824\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.10, NNZs: 211632, Bias: -1.000561, T: 366030, Avg. loss: 0.097800\n",
      "Total training time: 0.49 seconds.\n",
      "Convergence after 7 epochs took 0.49 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.32, NNZs: 107358, Bias: -1.008852, T: 52290, Avg. loss: 0.110482\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.16, NNZs: 142279, Bias: -1.006029, T: 104580, Avg. loss: 0.109008\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.12, NNZs: 166952, Bias: -1.004507, T: 156870, Avg. loss: 0.108781\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.10, NNZs: 183937, Bias: -1.003729, T: 209160, Avg. loss: 0.108676\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.09, NNZs: 197844, Bias: -1.003332, T: 261450, Avg. loss: 0.108621\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.09, NNZs: 207518, Bias: -1.002781, T: 313740, Avg. loss: 0.108581\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.08, NNZs: 215146, Bias: -1.002960, T: 366030, Avg. loss: 0.108554\n",
      "Total training time: 0.57 seconds.\n",
      "Convergence after 7 epochs took 0.57 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.14, NNZs: 95496, Bias: -1.004871, T: 52290, Avg. loss: 0.104734\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.96, NNZs: 128051, Bias: -1.002413, T: 104580, Avg. loss: 0.103170\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.90, NNZs: 157383, Bias: -1.002127, T: 156870, Avg. loss: 0.102979\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.87, NNZs: 182502, Bias: -1.001649, T: 209160, Avg. loss: 0.102892\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.86, NNZs: 203821, Bias: -1.001194, T: 261450, Avg. loss: 0.102835\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.85, NNZs: 221238, Bias: -1.000995, T: 313740, Avg. loss: 0.102798\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.84, NNZs: 235385, Bias: -1.000858, T: 366030, Avg. loss: 0.102774\n",
      "Total training time: 0.52 seconds.\n",
      "Convergence after 7 epochs took 0.53 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.34, NNZs: 121852, Bias: -1.005450, T: 52290, Avg. loss: 0.135132\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.15, NNZs: 162423, Bias: -1.003826, T: 104580, Avg. loss: 0.133190\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.10, NNZs: 194862, Bias: -1.003053, T: 156870, Avg. loss: 0.132925\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.07, NNZs: 219547, Bias: -1.002162, T: 209160, Avg. loss: 0.132803\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.06, NNZs: 237549, Bias: -1.001621, T: 261450, Avg. loss: 0.132731\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.05, NNZs: 251791, Bias: -1.001678, T: 313740, Avg. loss: 0.132685\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.05, NNZs: 263312, Bias: -1.001325, T: 366030, Avg. loss: 0.132651\n",
      "Total training time: 0.52 seconds.\n",
      "Convergence after 7 epochs took 0.53 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.35, NNZs: 105203, Bias: -1.007617, T: 52290, Avg. loss: 0.115210\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.19, NNZs: 137576, Bias: -1.004825, T: 104580, Avg. loss: 0.113561\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.14, NNZs: 163347, Bias: -1.003675, T: 156870, Avg. loss: 0.113339\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.12, NNZs: 182488, Bias: -1.002843, T: 209160, Avg. loss: 0.113239\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.11, NNZs: 196003, Bias: -1.002252, T: 261450, Avg. loss: 0.113171\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.11, NNZs: 207297, Bias: -1.002264, T: 313740, Avg. loss: 0.113135\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.10, NNZs: 216012, Bias: -1.002120, T: 366030, Avg. loss: 0.113104\n",
      "Total training time: 0.52 seconds.\n",
      "Convergence after 7 epochs took 0.52 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.27, NNZs: 124385, Bias: -1.006307, T: 52290, Avg. loss: 0.109734\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.10, NNZs: 166228, Bias: -1.004718, T: 104580, Avg. loss: 0.108213\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.06, NNZs: 196771, Bias: -1.003446, T: 156870, Avg. loss: 0.107997\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.04, NNZs: 220142, Bias: -1.002390, T: 209160, Avg. loss: 0.107894\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.03, NNZs: 237720, Bias: -1.002268, T: 261450, Avg. loss: 0.107838\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.02, NNZs: 252057, Bias: -1.002165, T: 313740, Avg. loss: 0.107801\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.02, NNZs: 262263, Bias: -1.002074, T: 366030, Avg. loss: 0.107772\n",
      "Total training time: 0.54 seconds.\n",
      "Convergence after 7 epochs took 0.54 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.10, NNZs: 94977, Bias: -1.005003, T: 52290, Avg. loss: 0.098604\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.93, NNZs: 126849, Bias: -1.003100, T: 104580, Avg. loss: 0.097256\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.87, NNZs: 156185, Bias: -1.001833, T: 156870, Avg. loss: 0.097075\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.84, NNZs: 180046, Bias: -1.001456, T: 209160, Avg. loss: 0.096991\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.83, NNZs: 201852, Bias: -1.001534, T: 261450, Avg. loss: 0.096940\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.82, NNZs: 219515, Bias: -1.000821, T: 313740, Avg. loss: 0.096905\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.82, NNZs: 234353, Bias: -1.000512, T: 366030, Avg. loss: 0.096881\n",
      "Total training time: 0.56 seconds.\n",
      "Convergence after 7 epochs took 0.56 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.47, NNZs: 149473, Bias: -1.004359, T: 52290, Avg. loss: 0.149622\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.27, NNZs: 202286, Bias: -1.002664, T: 104580, Avg. loss: 0.147619\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.22, NNZs: 244224, Bias: -1.001556, T: 156870, Avg. loss: 0.147320\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.19, NNZs: 277923, Bias: -1.001729, T: 209160, Avg. loss: 0.147183\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.18, NNZs: 301888, Bias: -1.001067, T: 261450, Avg. loss: 0.147104\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.18, NNZs: 319561, Bias: -1.000602, T: 313740, Avg. loss: 0.147049\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.17, NNZs: 333967, Bias: -1.000567, T: 366030, Avg. loss: 0.147010\n",
      "Total training time: 0.54 seconds.\n",
      "Convergence after 7 epochs took 0.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:    9.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabular...\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=42,\n",
       "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                               verbose=1, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy the pipeline and train the model\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model using the test data\n",
    "predicted = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33664805323950125\n"
     ]
    }
   ],
   "source": [
    "print (np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                            1A       0.29      0.22      0.25       630\n",
      "          1YearDailyAudioBible       0.36      0.55      0.43       633\n",
      "                   2DopeQueens       0.18      0.17      0.17       658\n",
      "               30For30Podcasts       0.37      0.45      0.41       646\n",
      "                       48Hours       0.28      0.22      0.25       601\n",
      "                     60Minutes       0.29      0.26      0.28       593\n",
      "       83WeekswithEricBischoff       0.48      0.68      0.57       704\n",
      "                   99Invisible       0.25      0.17      0.21       702\n",
      "         AccidentalTechPodcast       0.41      0.57      0.48       729\n",
      "                       Accused       0.36      0.49      0.41       563\n",
      "                 AceOnTheHouse       0.31      0.31      0.31       584\n",
      "               AdamCarollaShow       0.37      0.30      0.33       680\n",
      "                 AliceIsntDead       0.38      0.48      0.42       699\n",
      "AlisonRosenIsYourNewBestFriend       0.23      0.09      0.12       719\n",
      "             BehindtheBastards       0.30      0.25      0.27       901\n",
      "                KnowledgeFight       0.37      0.43      0.40       768\n",
      "                   PlanetMoney       0.37      0.35      0.36       710\n",
      "                PodSaveAmerica       0.17      0.13      0.15       578\n",
      "      SeincastASeinfeldPodcast       0.30      0.28      0.29       975\n",
      "\n",
      "                      accuracy                           0.34     13073\n",
      "                     macro avg       0.32      0.34      0.32     13073\n",
      "                  weighted avg       0.32      0.34      0.32     13073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted, target_names=docs_to_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
